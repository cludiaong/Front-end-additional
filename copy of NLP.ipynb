{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cludiaong/Front-end-additional/blob/master/copy%20of%20NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ly-gUf5wLRnj"
      },
      "outputs": [],
      "source": [
        "!pip install -q snscrape==0.3.4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import library\n",
        "import datetime as dt\n",
        "import re\n",
        "import string\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "import tweepy\n",
        "import pandas as pd\n",
        "import ast\n",
        "pd.options.mode.chained_assignment = None\n",
        "import numpy as np\n",
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set(style = 'whitegrid')\n",
        "\n",
        "\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "!pip install Sastrawi\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "from multiprocessing import Pool\n",
        "from functools import partial\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Dense, Dropout, LSTM\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "z96YBZCALYEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "a9eGcvbELah2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#menentukan maximal tweets yang akan diambil\n",
        "maxTweets = 5000\n",
        "tweets_list2 = []\n",
        "\n",
        "#mencari data tweets sesuai keyword\n",
        "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('vaksin covid since:2020-03-31 until:2020-12-31 lang:id').get_items()):\n",
        "    if i>maxTweets:\n",
        "      break\n",
        "    tweets_list2.append([tweet.date,tweet.content, tweet.username])"
      ],
      "metadata": {
        "id": "tqB4GNLVLcl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df2 = pd.DataFrame(tweets_list2, columns=['Datetime','Text', 'Username'])\n",
        "\n",
        "tweets_df2"
      ],
      "metadata": {
        "id": "vP0tG2eZLecc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df2.to_csv('drive/My Drive/5000dataset.csv', sep=',', index=False)\n"
      ],
      "metadata": {
        "id": "1vXiIkZbLgoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = pd.read_csv('drive/My Drive/5000dataset.csv')\n",
        "tweets"
      ],
      "metadata": {
        "id": "F8rzmJQuLiZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/evanmartua34/Twitter-COVID19-Indonesia-Sentiment-Analysis---Lexicon-Based/raw/master/cleaning_source/update_combined_slang_words.txt\n",
        "!ls"
      ],
      "metadata": {
        "id": "kta957rALki7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download data untuk tokenisasi dan penghapusan stop words\n",
        "\n",
        "slangJson = {}  #mendefinsikan dictionary kosong yang akan dipopulate\n",
        "                #dengan data kata-kata slang dari file update_combined_slang_words.txt\n",
        "\n",
        "# parse file menjadi sebuah python Dictionary\n",
        "with open('update_combined_slang_words.txt', 'r') as slangFile:\n",
        "  slangJson = ast.literal_eval(slangFile.read())\n",
        "\n",
        "print(slangJson)\n",
        "print(slangJson['@'])"
      ],
      "metadata": {
        "id": "PuLvSWftLnNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "contoh = \"org2 pd gak percaya covid\"\n",
        "contoh = word_tokenize(contoh)\n",
        "\n",
        "for i in range(len(contoh)):\n",
        "  if contoh[i] in slangJson:\n",
        "    contoh[i] = slangJson[contoh[i]]\n",
        "\n",
        "print(contoh)"
      ],
      "metadata": {
        "id": "Cujm6_z5LpWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gkounter = 0 # menggunakan variabel ini untuk mencatat berapa banyak data\n",
        "             # yang sudah diproses oleh fungsi-fungsi dibawah ini\n",
        "            \n",
        "def cleaningText(text):\n",
        "    global gkounter\n",
        "    text = re.sub(r'@[A-Za-z0-9]+', '', str(text)) # hapus mentions\n",
        "    text = re.sub(r'#[A-Za-z0-9]+', '', str(text)) # hapus hashtag\n",
        "    #Removes unicode strings like \"\\u002c\" and \"x96\" \n",
        "    text = re.sub(r'(\\\\u[0-9A-Fa-f]+)',r'', str(text))       \n",
        "    text = re.sub(r'[^\\x00-\\x7f]',r'',str(text))\n",
        "    text = re.sub(r'RT[\\s]', '',  str(text)) # hapus RT\n",
        "    text = re.sub(r'(http\\S+)|(www\\.[^\\s]+)', '',  str(text))# hapus link\n",
        "    #Remove any @Username\n",
        "    text = re.sub('@[^\\s]+','',str(text))\n",
        "    #Remove additional white spaces\n",
        "    text = re.sub('[\\s]+', ' ', str(text))\n",
        "    text = re.sub('[\\n]+', ' ', str(text))\n",
        "    text = re.sub(r'[0-9]+', '', str(text)) # hapus angka\n",
        "    text = re.sub(r'[^0-9A-Za-z \\t]|(\\w+:\\/\\/\\S+)+','', str (text)) # hapus emoticon\n",
        "    text = text.replace('\\n', ' ')# ganti baris baru dengan spasi\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation)) # hapus semua tanda baca\n",
        "    text = text.strip(' ') # hapus spasi kosong yang ada di awal dan di akhir teks\n",
        "    print(f\"cleaned {gkounter} text\")\n",
        "    gkounter += 1\n",
        "    return text"
      ],
      "metadata": {
        "id": "ZoUB1XVpLs7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_sample= \"How :) to take control of your #debt https://personal.vanguard.com/us/insights/saving-investing/debt-management.#Best advice for #family #financial #success (@PrepareToWin)\"\n",
        "gkounter = 0\n",
        "tweet_sample = cleaningText(tweet_sample)\n",
        "tweet_sample\n"
      ],
      "metadata": {
        "id": "VY4RrsltLvXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gkounter = 0\n",
        "tweets['text_clean'] = tweets['Text'].apply(cleaningText)\n",
        "tweets['text_clean'].head()"
      ],
      "metadata": {
        "id": "cCN6M_DeLy19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contoh1 = '@abcd saya suka masak'\n",
        "contoh1 = re.sub(r'@[A-Za-z0-9]+', '', str(contoh1)) # hapus mentions\n",
        "contoh1"
      ],
      "metadata": {
        "id": "ig9RdQeHL3hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gkounter = 0 # menggunakan variabel ini untuk mencatat berapa banyak data\n",
        "             # yang sudah diproses oleh fungsi-fungsi dibawah ini\n",
        "            \n",
        "def cleaningText(text):\n",
        "    global gkounter\n",
        "    text = re.sub(r'@[A-Za-z0-9]+', '', str(text)) # hapus mentions\n",
        "    text = re.sub(r'#[A-Za-z0-9]+', '', str(text)) # hapus hashtag\n",
        "    text = re.sub(r'RT[\\s]', '',  str(text)) # hapus RT\n",
        "    text = re.sub(r\"http\\S+\", '',  str(text))# hapus link\n",
        "    text = re.sub(r'[0-9]+', '', str(text)) # hapus angka\n",
        "    text = re.sub(r'[^0-9A-Za-z \\t]|(\\w+:\\/\\/\\S+)+','', str (text)) # hapus emoticon\n",
        "    text = text.replace('\\n', ' ')# ganti baris baru dengan spasi\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation)) # hapus semua tanda baca\n",
        "    text = text.strip(' ') # hapus spasi kosong yang ada di awal dan di akhir teks\n",
        "    print(f\"cleaned {gkounter} text\")\n",
        "    gkounter += 1\n",
        "    return text\n",
        "\n",
        "#casefolding\n",
        "def casefoldingText(text): # mengubah huruf menjadi huruf kecil\n",
        "    global gkounter\n",
        "    text = text.lower() \n",
        "    print(f\"casefolded {gkounter} text\")\n",
        "    gkounter += 1\n",
        "    return text\n",
        "\n",
        "#tokenizing\n",
        "def tokenizingText(text): # memisahkan string menjadi kumpulan token\n",
        "    global gkounter\n",
        "    text = word_tokenize(text) \n",
        "    print(f\"tokenized {gkounter} text\")\n",
        "    gkounter += 1\n",
        "    return text\n",
        "\n",
        "#normalisasi slang words \n",
        "def normalizeSlangWords(text):  #normalisasi kata alay/slang menjadi kata baku bahasa indonesia\n",
        "    global gkounter\n",
        "    for i in range(len(text)):\n",
        "      if text[i] in slangJson:\n",
        "        text[i] = slangJson[text[i]]\n",
        "    print(f\"normalized slang words {gkounter} text\")\n",
        "    gkounter += 1\n",
        "    return text\n",
        "\n",
        "#normalisasi slang word - filtering text\n",
        "#https://github.com/masdevid/ID-Stopwords \n",
        "def filteringText(text): # hapus stop words\n",
        "    global gkounter\n",
        "    listStopwords = set(stopwords.words('indonesian'))\n",
        "    filtered = []\n",
        "    for txt in text:\n",
        "        if txt not in listStopwords:\n",
        "            filtered.append(txt)\n",
        "    text = filtered \n",
        "    print(f\"filtered {gkounter} text\")\n",
        "    gkounter += 1\n",
        "    return text\n",
        "\n",
        "#stemming\n",
        "def stemmingText(text): # buat semua kata menjadi kata baku (contoh: 'melelahkan' => 'lelah')\n",
        "    global gkounter\n",
        "    factory = StemmerFactory()\n",
        "    stemmer = factory.create_stemmer()\n",
        "    text = [stemmer.stem(word) for word in text]\n",
        "    print(f\"stemmed {gkounter} text\")\n",
        "    gkounter += 1\n",
        "    return text\n",
        "\n",
        "def toSentence(list_words): # fungsi untuk menyambung kembali token-token kata menjadi satu kalimat\n",
        "    sentence = ' '.join(word for word in list_words)\n",
        "    return sentence\n"
      ],
      "metadata": {
        "id": "VlBRZDOVL7t5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}